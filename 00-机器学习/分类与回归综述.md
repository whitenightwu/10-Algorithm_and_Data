# 回归与分类
回归和分类的相同点：总的来说两个问题本质上都是一致的，就是模型的拟合（匹配）。
不同点：分类指的是预测值是有限个离散值的情况；而回归指的是预测值是连续值的情况。

## 使用流程
线性回归和分类问题都有以下几个步骤,

1）如何选取一个合理的模型(线性的，or 非线性的(e.g. 阶跃函数， 高斯函数)).

2）制造一个”“美好”“的误差函数 (可以评估拟合程度，而且还是convex函数)；

3）采取一切可能的技术(e.g. 导数下降法，解极值方程法) 求出最好的模型参数。


## 使用分类还是回归？
回归问题一般都可以转换成为分类问题，另外分类问题一般更简单，效果更好，但也损失了更多的信息。

一般在下面情况下，会将回归转换成分类：
1）数据限制太大(数据量少),做回归的效果很差；
2）对精度的要求较低。例如：做股票预测不需要知道跌多少或者涨多少，只需要知道跌或者涨就可以了，这样就转换成了二分类问题。

--------
# 分类（Classification）
有两种比较有意思的分类：层次分类（hierarchical classification）和多标签分类(multi-label classification)。

1）层次分类（hierarchical classification）：层次假设各个类别之间或多或少地存在某种关系。并非所有的错误都同等严重 —— 错误地归入相近的类别，比距离更远的类别要好得多。一个早期的例子来自卡尔·林奈，他发明了沿用至今的物种分类法。

2）多标签分类：预测非互斥分类的任务。想象一下，人们可能会把多个标签同时标注在自己的某篇技术类博客文章上，例如“机器学习”、“科技”、“编程语言”、“云计算”、“安全与隐私”和“AWS”。这里面的标签其实有时候相互关联，比如“云计算”和“安全与隐私”。


------
# 回归（Regression）
回归是在自变量和需要预测的变量之间构建一个模型，并使用迭代的方法逐渐降低预测值和真实值之间的误差。回归方法是统计机器学习的一种。
一言蔽之，回归就是对连续值的预测。

## 常用的回归算法
```
 决策林回归
 
 神经网络回归
 
 贝叶斯回归
 
 Ordinary Least Squares（最小二乘法）
 
 Logistic Regression（逻辑斯底回归）
 
 Stepwise Regression（逐步回归）
 
 Multivariate Adaptive Regression Splines（多元自适应回归样条法）
 
 Locally Estimated Scatterplot Smoothing（局部加权散点平滑法）
```

## 什么是逻辑回归
逻辑回归结合“逻辑”和“回归”，我认为它是对离散值的预测，而不是对连续值的预测。
总的来说，逻辑回归是预测分类值的过程，预测的可能是简单的二分类预测值，也可能是复杂的多分类预测值。

------
 # 参考资料
 https://blog.csdn.net/ppn029012/article/details/8775597
李沐：机器学习简介