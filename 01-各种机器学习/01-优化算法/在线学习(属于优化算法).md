# 在线学习（属于优化算法）


## 在线学习（online learning）与离线学习（offline learning）
在线学习（online learning）是相对于离线学习（offline learning）的概念。

1）所谓离线学习，也称为batch learning，意思就是一个batch训练完才更新权重。
优点：优化过程稳定，不会因为偶然的错误数据把模型带向极端。
缺点：当数据规模大时，其计算复杂度高、响应慢，无法用于实时性要求高的应用。
传统的机器学习算法是批量模式的，假设所有的训练数据预先给定，通过最小化定义在所有训练数据上的经验误差得到分类器。

2）在线学习
优点：比较快；当训练数据是持续到来的时候，利用一个训练样本更新当前的模型，大大降低了学习算法的空间复杂度和时间复杂度，实时性强。
缺点：优化时不稳定，损失有较高的方差。

## 定义
经过几十年的发展，在线学习已经形成了一套完备的理论。

在线学习可以定义为学习器和对手之间的博弈：在每一个时刻t，学习器从决策空间选择一个决策w_t，同时对手选择一个损失函数f_t()，这样学习器在当前时刻遭受损失f_t(w_t)；根据遭受的损失，学习器对当前的决策进行更新，从而决定下一时刻的决策。学习器的目的是最小化T个时刻的累计损失。

在分析在线学习算法的效果时，我们通常将在线学习的累计误差与批量学习的累计误差相比较，将其差值称为遗憾（regret）。因此，在线学习最小化累计误差也等价于最小化遗憾，遗憾的上界也就成为衡量在线学习算法性能的标准。


## 根据观测信息的不同分为两类
根据学习器在学习过程中观测信息的不同，在线学习还可以再进一步分为两种：

### 1）完全信息下的在线学习
假设学习器可以观测到完整的损失函数。如果学习器可以观测到训练样本，该问题就属于完全信息下的在线学习，因为基于训练样本就可以定义完整的分类误差函数，所以相对容易。
由于损失函数是已知的，因此可以计算其梯度、海森(Hessian)矩阵等信息，辅助学习器更新决策。
已有大量成熟算法，但在许多现实应用中，学习器能够观测到损失函数的这种假设并不成立，使得这些算法不能被直接应用。

常见算法：在线梯度下降是针对该设定最常用的算法，其他常用的学习算法还包括在线牛顿法、正则化最优决策法、在线核学习。

实际应用：完全信息下的在线学习主要被应用到在线分类、在线物体识等等反馈充分的问题中，主要目的是降低训练复杂度，提高算法实时性。以在线广告推荐为例，当学习器向用户推荐广告后，可以得到用户是否点击该广告的反馈，但是用户产生该反馈的机制学习器并不知晓。这种情况就是赌博机在线学习的研究范畴。


### 2）赌博机在线学习
假设学习器只能观测到损失函数在当前决策上的数值，即 f_t(w_t)。如果学习器只能观测到分类误差而看不到训练样本，该问题就属于赌博机在线学习。
由于观测信息的不同，针对这两种设定的学习算法也存在较大差异，其应用场景也不同。

一方面，为了准确地估计损失函数的结构，学习器需要尝试更多的新决策；而另一方面，为了最小化遗憾，学习器又倾向于选择能最小化损失函数的决策。故遗憾上界较大；并且难以设计通用的学习算法，需要针对不同的函数类型、不同的随机假设设计不同的算法。

实际应用：最早用于多臂赌博机问题。赌博机在线学习主要应用于商品推荐、、广告投放、网络路由等反馈受限问题中，主要目的是支持模糊决策，在探索和利用之间寻找最优的平衡。

### 两类算法前沿研究
1）完全信息下的在线学习研究前沿包括非凸函数在线学习、非线性函数在线学习等问题。
2）赌博机在线学习的研究热点主要围绕如何将算法和理论拓展到弱反馈场景，比如基于比较的赌博机。

# 参考资料
https://blog.csdn.net/a133521741/article/details/79221015
https://baijiahao.baidu.com/s?id=1594337146635999109&wfr=spider&for=pc
https://blog.csdn.net/hellozhxy/article/details/82703561
